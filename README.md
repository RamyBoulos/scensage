

# SceneSage

**SceneSage** is a Python-based command-line tool for enriching video subtitles with AI-generated scene annotations. It processes `.srt` subtitle files, segments them into scenes, and uses Large Language Models (LLMs) to annotate each scene with a summary, characters, mood, and cultural references.

Supports both remote inference via Hugging Face API (default) and local inference using the Mistral 7B model via Ollama.

Repository: [github.com/RamyBoulos/scensage](https://github.com/RamyBoulos/scensage)

---

## Features

- ðŸŽ¬ **Scene Segmentation** â€“ Groups subtitles into coherent scenes using LLM or time-gap strategy.
- ðŸ¤– **LLM Annotation** â€“ Adds summaries, character info, mood, and cultural references to each scene.
- ðŸ”Œ **Hugging Face & Ollama** â€“ Use Hugging Face models (default) or local Mistral via Ollama.
- ðŸ³ **Docker Support** â€“ Run SceneSage in a containerized environment.
- ðŸ› ï¸ **Makefile Shortcuts** â€“ Convenient commands for testing and running locally or in Docker.

---

## Installation

### 1. Clone the Repository

```bash
git clone https://github.com/RamyBoulos/scensage.git
cd scensage
```

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

> **Note**: This does not install a CLI command globally. Run via `python3 -m scenesage.scenesage` or use the Makefile/Docker.

---

## Usage

### ðŸ”§ Set Up Environment Variables

- For Hugging Face API:

```bash
export HUGGINGFACEHUB_API_TOKEN=your_hf_token
export USE_LOCAL=none
```

- For local Ollama inference:

```bash
export USE_LOCAL=mistral
ollama run mistral  # Make sure Mistral is pulled and running
```

---

### â–¶ï¸ Command-Line Example

Run directly using Python:

```bash
python3 -m scenesage.scenesage path/to/input.srt --output scenes.json --model mistralai/Mixtral-8x7B-Instruct-v0.1
```

Or use the Makefile shortcut (with environment variables and defaults pre-configured):

```bash
make hf-run SRT=path/to/input.srt OUT=scenes.json MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
```

---

### ðŸ“„ Sample Output (from `scenes.json` and debug prints)

Below are two annotated scenes from the demo input file `tests/data/plan9.srt`. These were generated during a local run using Mistral via Ollama. You can also see them echoed in the `RAW MISTRAL RESPONSE` debug printouts.

#### ðŸŽ¬ Scene 1 â€” Opening Narration (Voiceover)

```json
{
  "summary": "Scene begins with a dramatic voiceover introducing a mysterious story involving extraterrestrial grave robbers and the future. The narrator promises to reveal the full story based on secret testimonies of survivors.",
  "characters": ["Narrator"],
  "mood": "Suspenseful, Mysterious",
  "cultural_refs": []
}
```

#### âœˆï¸ Scene 4 â€” Cemetery + Pilot Banter

```json
{
  "summary": "Gravediggers begin their work at a cemetery as two pilots engage in a light-hearted radio conversation with air traffic control, referencing Burbank Airport (San Fernando Valley) and the possibility of one pilot sleeping.",
  "characters": ["Gravedigger", "Danny (pilot)", "Jeff (pilot)", "Mac (air traffic control)"],
  "mood": "lighthearted, humorous",
  "cultural_refs": ["aviation", "geographical location - San Fernando Valley"]
}
```

> These are two of the 20 annotated scenes stored in `scenes.json` after running the tool. Each scene includes summary, character extraction, mood classification, and cultural references (when found).

---

### ðŸ› ï¸ Makefile Shortcuts

The Makefile includes convenient commands for different use cases.

#### Run with Hugging Face API (default)

```bash
make hf-run
```

#### Run with local Mistral via Ollama

```bash
make mistral-run
```

#### Segmentation strategies

```bash
make run-llm         # LLM-based segmentation with HF
make run-gap         # Time-gap segmentation with HF
make mistral-llm     # LLM segmentation + local Mistral
make mistral-gap     # GAP segmentation + local Mistral
```

---

### ðŸ³ Docker Usage

#### Build Docker Image

```bash
make docker-build
```

#### Run with Hugging Face

```bash
make docker-hf-run
```

#### Run with local Mistral (Ollama must be running on host)

```bash
make docker-mistral-run
```

> The Docker container communicates with the Ollama host at `host.docker.internal`. You must have Mistral running via Ollama **outside Docker**.

---

## File Structure

```
scenesage/
â”œâ”€â”€ scenesage/             # Core source code
â”‚   â”œâ”€â”€ scenesage.py       # CLI entry point
â”‚   â”œâ”€â”€ annotator.py       # LLM scene annotation
â”‚   â”œâ”€â”€ parser.py          # SRT parsing and scene segmentation
â”‚   â”œâ”€â”€ utils.py           # Timestamp parsing, JSON extraction
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/                 # Unit tests
â”œâ”€â”€ Dockerfile             # Docker config
â”œâ”€â”€ Makefile               # Run shortcuts
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # This file
```

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## Author

**Ramy Boulos**  
[github.com/RamyBoulos](https://github.com/RamyBoulos)