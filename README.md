

# SceneSage

**SceneSage** is a Python-based command-line tool for enriching video subtitles with AI-generated scene annotations. It processes `.srt` subtitle files, segments them into scenes, and uses Large Language Models (LLMs) to annotate each scene with a summary, characters, mood, and cultural references.

Supports both remote inference via Hugging Face API (default) and local inference using the Mistral 7B model via Ollama.

Repository: [github.com/RamyBoulos/scensage](https://github.com/RamyBoulos/scensage)

---

## Features

- ðŸŽ¬ **Scene Segmentation** â€“ Groups subtitles into coherent scenes using LLM or time-gap strategy.
- ðŸ¤– **LLM Annotation** â€“ Adds summaries, character info, mood, and cultural references to each scene.
- ðŸ”Œ **Hugging Face & Ollama** â€“ Use Hugging Face models (default) or local Mistral via Ollama.
- ðŸ³ **Docker Support** â€“ Run SceneSage in a containerized environment.
- ðŸ› ï¸ **Makefile Shortcuts** â€“ Convenient commands for testing and running locally or in Docker.

---

## Installation

### 1. Clone the Repository

```bash
git clone https://github.com/RamyBoulos/scensage.git
cd scensage
```

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

> **Note**: This does not install a CLI command globally. Run via `python3 -m scenesage.scenesage` or use the Makefile/Docker.

---

## Usage

### ðŸ”§ Set Up Environment Variables

- For Hugging Face API:

```bash
export HUGGINGFACEHUB_API_TOKEN=your_hf_token
export USE_LOCAL=none
```

- For local Ollama inference:

```bash
export USE_LOCAL=mistral
ollama run mistral  # Make sure Mistral is pulled and running
```

---

### â–¶ï¸ Command-Line Example

```bash
python3 -m scenesage.scenesage path/to/input.srt --output scenes.json --model mistralai/Mixtral-8x7B-Instruct-v0.1
```

---

### ðŸ› ï¸ Makefile Shortcuts

The Makefile includes convenient commands for different use cases.

#### Run with Hugging Face API (default)

```bash
make hf-run
```

#### Run with local Mistral via Ollama

```bash
make mistral-run
```

#### Segmentation strategies

```bash
make run-llm         # LLM-based segmentation with HF
make run-gap         # Time-gap segmentation with HF
make mistral-llm     # LLM segmentation + local Mistral
make mistral-gap     # GAP segmentation + local Mistral
```

---

### ðŸ³ Docker Usage

#### Build Docker Image

```bash
make docker-build
```

#### Run with Hugging Face

```bash
make docker-hf-run
```

#### Run with local Mistral (Ollama must be running on host)

```bash
make docker-mistral-run
```

> The Docker container communicates with the Ollama host at `host.docker.internal`. You must have Mistral running via Ollama **outside Docker**.

---

## File Structure

```
scenesage/
â”œâ”€â”€ scenesage/             # Core source code
â”‚   â”œâ”€â”€ scenesage.py       # CLI entry point
â”‚   â”œâ”€â”€ annotator.py       # LLM scene annotation
â”‚   â”œâ”€â”€ parser.py          # SRT parsing and scene segmentation
â”‚   â”œâ”€â”€ utils.py           # Timestamp parsing, JSON extraction
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/                 # Unit tests
â”œâ”€â”€ Dockerfile             # Docker config
â”œâ”€â”€ Makefile               # Run shortcuts
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # This file
```

---

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## Author

**Ramy Boulos**  
[github.com/RamyBoulos](https://github.com/RamyBoulos)